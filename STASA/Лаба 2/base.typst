#set text(font: "Times New Roman", size: 14pt)
#set heading(numbering: none)
#show heading: it => {
  set text(font: "Times New Roman", size: 14pt, weight: "bold")
  it
}

#set par(first-line-indent: 1.25cm, justify: true)

= Ход выполнения работы

== Задание 1. Оценка параметров экспоненциальной модели при аддитивном нормальном шуме

Задана истинная функция

$ y(x) = 3 dot exp(0.5 dot x) $

Было сгенерировано n = 500 равноотстоящих точек $x in [0.1, 10]$.

К истинным значениям z = y(x) добавлен нормальный шум с дисперсией $sigma = 0.1 dot z$:

$ z_"noised_normal" = z + ε, quad ε in N(0, σ) $

Оценка параметров модели ŷ(x) = a exp(b x) выполнена двумя способами:

1. Линеаризация + МНК (логарифмирование $ln z = ln a + b dot x$ и обычный МНК):

  $ hat(a) = exp(hat(θ)_0) = 2.9779315, quad hat(b) = 0.50064383 $

  Итоговая модель: ŷ(x) = 2.978 exp(0.501 x)

2. Нелинейный МНК (scipy.optimize.curve_fit):

  $ hat(a) = 3.13141155, quad hat(b) = 0.4947136 $

Метрики качества (MSE):

- Линеаризация + МНК: MSE = 111.32
- Нелинейный МНК: MSE = 95.84

== Задание 2. Оценка параметров той же модели при аддитивном лог-нормальном шуме

Истинным значениям z = y(x) добавлен лог-нормальный шум:

$ ε ∼ "lnN"(0, 1.25), quad z_"noised_lognormal" = z + ε $

Оценка параметров модели ŷ(x) = a exp(b x) снова выполнена двумя способами:

1. Линеаризация + МНК:

  $ hat(a) = exp(hat(θ)_0) = 3.8359922, quad hat(b) = 0.4701052 $

  Итоговая модель: ŷ(x) = 3.836 exp(0.470 x)

2. Нелинейный МНК (curve_fit):

  $ hat(a) = 3.3233011, quad hat(b) = 0.48970837 $

Метрики качества (MSE):

- Линеаризация + МНК: MSE = 258.47
- Нелинейный МНК: MSE = 241.13

== Анализ остатков

Для обоих типов шума построены графики остатков (residuals = z_noised – ŷ(x)) в зависимости от x.

При нормальном шуме остатки после нелинейного МНК имеют практически случайный характер и меньший разброс, тогда как после линеаризации наблюдается слабая систематическая зависимость остатков от x.

При лог-нормальном шуме остатки после линеаризации демонстрируют выраженную систематическую ошибку (занижение на малых x, завышение на больших). Нелинейный МНК даёт значительно более однородные остатки, хотя разброс больше из-за тяжёлых хвостов лог-нормального распределения.

== Выводы по лабораторной работе

1. При аддитивном нормальном шуме оба метода дают близкие оценки параметров, но нелинейный МНК обеспечивает меньшую ошибку MSE и более случайные остатки.

2. При аддитивном лог-нормальном шуме (гетероскедастичность и асимметрия) линеаризация приводит к сильному систематическому смещению оценок и значительно худшему качеству аппроксимации.

3. Линеаризация с обычным МНК может использоваться для быстрого получения начального приближения, однако при нарушении предположений классического МНК в исходных единицах (постоянная дисперсия, нормальность ошибок) прямой нелинейный метод даёт существенно лучшие результаты.
